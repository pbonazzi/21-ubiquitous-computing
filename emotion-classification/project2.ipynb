{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import scipy\n",
    "from scipy import signal, stats\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.signal import find_peaks, butter, filtfilt, iirnotch, savgol_filter, welch, lfilter, freqz, argrelextrema\n",
    "from scipy.stats import iqr\n",
    "from scipy.integrate import simps\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "import cvxopt as cv\n",
    "import cvxopt.solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() :\n",
    "    \"\"\"Extract all the data from the pickle files in the folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Return\n",
    "    ------\n",
    "    data : 1d-array\n",
    "      list of dictionaries\n",
    "    \"\"\"  \n",
    "        \n",
    "    data=[]\n",
    "    for i in range (1,45):\n",
    "        if i < 10 :\n",
    "            div='0' \n",
    "        else : \n",
    "            div=''\n",
    "        with open(\"data/participant_\"+div+str(i)+\".pkl\" ,\"rb\") as f : \n",
    "            data.append(pickle.load(f))\n",
    "    return data\n",
    "\n",
    "data=load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction and data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentQ1Q3(x, mean, std):\n",
    "    \"\"\"Return the percentage of times the value is above μ + σ and below μ − σ of the corresponding feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1d-array\n",
    "      Input feature\n",
    "    mean : float\n",
    "      Mean\n",
    "    std : float\n",
    "      Standard deviation\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    return 1 : float\n",
    "      Percentage of times the value is below μ − σ\n",
    "    return 2 : float\n",
    "      Percentage of times the value is above μ + σ\n",
    "    \"\"\" \n",
    "    \n",
    "    if len(x) == 0 :\n",
    "        \n",
    "        return 0,0\n",
    "    \n",
    "    idx_1 = [number for number in x if number < mean-std]\n",
    "\n",
    "    idx_3 = [number for number in x if number > mean+std] \n",
    "    \n",
    "    return round(len(idx_1)/len(x),3), round(len(idx_3)/len(x),3)\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "def last50Sec(time, data):\n",
    "    \n",
    "    \"\"\"Return the last 50 seconds of the signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1d-array\n",
    "      Input signal in the time-domain.\n",
    "    time : 1d-array\n",
    "      Time in milliseconds\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    data50 : 1d-array\n",
    "      Last 50 seconds of the input signal\n",
    "    time50 : 1d-array\n",
    "      Last 50 seconds of the input time\n",
    "    idx : int\n",
    "      First valid index in the original data trace\n",
    "    \"\"\"    \n",
    "    data50, time50, i = [], [], 1\n",
    "    \n",
    "    end = time[len(time)-i] - 50*1000\n",
    "    \n",
    "    while(time[len(time)-i] > end):\n",
    "        \n",
    "        idx=len(time)-i\n",
    "        \n",
    "        data50.append(data[idx])\n",
    "        time50.append(time[idx])\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "    data50 = data50[::-1]\n",
    "    time50 = time50[::-1]\n",
    "    \n",
    "    \n",
    "    return data50, time50, idx\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "def bandpower(data, sf, band, nfft=None, additional=False):\n",
    "    \"\"\"Compute the average, min/max, std of power of the signal x in a specific frequency band.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1d-array\n",
    "      Input signal in the time-domain.\n",
    "    sf : float\n",
    "      Sampling frequency of the data.\n",
    "    band : list\n",
    "      Lower and upper frequencies of the band of interest.\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    bp : float\n",
    "      Absolute band power.\n",
    "    \"\"\"\n",
    "    from scipy.signal import welch\n",
    "    from scipy.integrate import simps\n",
    "\n",
    "    band = np.asarray(band)\n",
    "    low, high = band\n",
    "    freqs, psd = welch(data, sf, nperseg=sf*15, noverlap=sf*10, nfft=nfft)\n",
    "\n",
    "    \n",
    "    # Find index of band in frequency vector\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "    \n",
    "    # Average - Integral approximation of the spectrum using parabola (Simpson's rule) \n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "    avg = round(simps(psd[idx_band], dx=freq_res),3)\n",
    "        \n",
    "    if additional == True : \n",
    "        \n",
    "        # Min/Max for additional feat\n",
    "        maxi=max(psd[idx_band])\n",
    "        mini=min(psd[idx_band])\n",
    "    \n",
    "        # Std\n",
    "        std=statistics.stdev(psd[idx_band])\n",
    "        \n",
    "        return maxi, mini, std, avg\n",
    "        \n",
    "    return avg    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECG Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadL1(data, clip):\n",
    "    \"\"\"Load lead 1 from the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "      dictionary of the participant.\n",
    "    clip : int\n",
    "      number of clip\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    time : 1d-array\n",
    "      The time series for the given clip when data is available.\n",
    "    l1 : 1d-array\n",
    "      The lead 1 series for the given clip when data is available.\n",
    "    fs : float\n",
    "      Sampling frequency.      \n",
    "    mean_negatives : float\n",
    "      Trimmed mean of data in the forth Cartesian quadrant.\n",
    "    \"\"\"\n",
    "    \n",
    "    nECG=data[\"recordings\"][clip]['ECG']\n",
    "    fs=data[\"FS_ECG\"]\n",
    "    \n",
    "    l1, time, negatives = [], [],[]\n",
    "    \n",
    "    for i in range (0,len(nECG)):\n",
    "        \n",
    "        if math.isnan(nECG[i][2]) or math.isnan(nECG[i][1]) or  math.isnan(nECG[i][0]) :\n",
    "            continue\n",
    "            \n",
    "        else :\n",
    "            lead1=nECG[i][2] - nECG[i][1]\n",
    "            l1.append(lead1)\n",
    "            time.append(nECG[i][0])\n",
    "            if lead1<0 : \n",
    "                negatives.append(lead1)\n",
    "    \n",
    "    numneg= len(negatives)\n",
    "    numtot= len(l1)\n",
    "    ratio = numneg/numtot\n",
    "    \n",
    "    mean_negatives=stats.trim_mean(negatives, 0.25)\n",
    "    \n",
    "    return time, l1, fs, mean_negatives\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "def paper_lowpass(data, mean_negatives):\n",
    "    \"\"\"Low pass filter using the scipy library\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1d-array\n",
    "      Input signal\n",
    "    cutoff : float\n",
    "      Cutoff frequency\n",
    "    fs : float\n",
    "      Sampling frequency\n",
    "    order : int\n",
    "      Order of the filter\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    y : 1d-array\n",
    "      Filtered signal\n",
    "    \"\"\"  \n",
    "    \n",
    "    \n",
    "    noise=[0]*len(data)\n",
    "    \n",
    "    noise[0]=data[0]\n",
    "    \n",
    "    alpha = 1/8\n",
    "    \n",
    "    for i in range (1, len(data)) : \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            noise[i]=noise[i-1]+alpha*data[i]-alpha*data[i-8]\n",
    "        \n",
    "        except IndexError:\n",
    "            \n",
    "            noise[i]=data[i]\n",
    "            \n",
    "    if stats.trim_mean(data, 0.25)<0 : \n",
    "        \n",
    "        filtered_data=np.array(data)-np.array(noise)-[mean_negatives]*len(data)\n",
    "        \n",
    "    else : \n",
    "        \n",
    "        filtered_data=np.array(data)-np.array(noise)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "def ArtifactsDetection(data, time):\n",
    "    \"\"\"Detect artifacts in the ECG data (alg from the lecture)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1-d array\n",
    "      lead 1\n",
    "    time : 1-d array\n",
    "      time\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    peaks : 1d-array\n",
    "      Detected peaks\n",
    "    peaks Valid : 1d-array\n",
    "      Valid peaks\n",
    "    \"\"\"\n",
    "    peaks, _ = find_peaks(data, distance =150, prominence=90, width=3)\n",
    "    \n",
    "    if len(peaks)==0 : \n",
    "        return [0,1,2],[0,0,0]\n",
    "    \n",
    "    beats=[0]*len(peaks)\n",
    "    \n",
    "    for i in range (1, len(peaks)) : \n",
    "        beats[i]=peaks[i]-peaks[i-1]\n",
    "        \n",
    "    MedianBeat=statistics.median(beats)\n",
    "    QD=iqr(beats)/2\n",
    "    MAD=(MedianBeat-2.9*QD)/3\n",
    "    MED=3.32*QD\n",
    "    CBD=(MAD+MED)/2\n",
    "\n",
    "    peaksValid=[0]*len(peaks)\n",
    "    Rnn_last=time[peaks[len(peaks)-2]]- time[peaks[len(peaks)-1]]\n",
    "\n",
    "    for i in range (2, len(peaks)-1) :   \n",
    "\n",
    "        Rnn=time[peaks[i]]- time[peaks[i-1]]#current\n",
    "        Rnn_minus1=time[peaks[i-1]]- time[peaks[i-2]]#previous\n",
    "        Rnn_plus1=time[peaks[i]]- time[peaks[i+1]]#successor\n",
    "        \n",
    "        if Rnn_minus1<=2000 and Rnn_minus1>=300 :\n",
    "            \n",
    "            if abs(Rnn-Rnn_minus1)<=CBD :\n",
    "                \n",
    "                peaksValid[i]=1\n",
    "                peaksValid[i-1]=1\n",
    "        else :\n",
    "            \n",
    "            if abs(Rnn-Rnn_last)<=CBD :\n",
    "                \n",
    "                peaksValid[i]=1\n",
    "                peaksValid[i-1]=1\n",
    "                \n",
    "            else :\n",
    "                \n",
    "                if abs(Rnn-Rnn_minus1)<=CBD and abs(Rnn-Rnn_plus1)<=CBD :\n",
    "                    \n",
    "                    peaksValid[i]=1\n",
    "                    peaksValid[i-1]=1\n",
    "                    \n",
    "    return peaks, peaksValid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "class ECG:      \n",
    "    \n",
    "    def __init__(self, number, clip):\n",
    "        \n",
    "        participant_data = data[number-1]\n",
    "        \n",
    "        self.time, self.l1, self.fs, mean_negatives = loadL1(participant_data, clip)\n",
    "\n",
    "        self.l1_filter = paper_lowpass(self.l1, mean_negatives)\n",
    "        \n",
    "        self.peaks, self.peaksValid = ArtifactsDetection(self.l1_filter, self.time)\n",
    "        \n",
    "        self.f, self.Pxx_den = signal.welch(self.l1_filter, self.fs, nperseg=self.fs*15, noverlap=self.fs*10, nfft=len(self.l1_filter))\n",
    "        \n",
    "        self.time50, self.l50, self.idx = last50Sec(self.time, self.l1_filter)\n",
    "\n",
    "    #####################################################################################################        \n",
    "     \n",
    "    def plot_PSD(self, band):\n",
    "        \n",
    "        f2, Pxx_den2 = signal.welch(self.l1, self.fs, nperseg=self.fs*15, noverlap=self.fs*10)\n",
    "        minFrequencies, maxFrequencies = band\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.semilogy(self.f, self.Pxx_den, label=\"Filtered\")\n",
    "        plt.semilogy(f2, Pxx_den2, label=\"Raw\")\n",
    "        plt.xlim([minFrequencies, maxFrequencies])\n",
    "        plt.legend()\n",
    "        plt.xlabel('Frequency [Hz]')\n",
    "        plt.ylabel('PSD')\n",
    "        plt.show()\n",
    "        \n",
    "    #####################################################################################################        \n",
    "     \n",
    "    def plot_ECG(self, band = [300000, 310000]):\n",
    "        \n",
    "        #minTime, maxTime = band\n",
    "\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(self.time, self.l1_filter, label ='L1 with low pass')\n",
    "        #plt.plot(self.time, self.l1, label ='Lead 1 - as is')\n",
    "        #plt.xlim([minTime, maxTime])\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time [ms]')\n",
    "        plt.ylabel('')\n",
    "        plt.show()\n",
    "        \n",
    "    #####################################################################################################        \n",
    "     \n",
    "    def invalidData(self):\n",
    "        \n",
    "        ##Percentage of invalid ECG\n",
    "        count=0\n",
    "        for i in range (1, len(self.peaksValid)-1) :\n",
    "            \n",
    "            if self.peaksValid[i]==0 :    \n",
    "                start_invalid=int((self.peaks[i-1]+self.peaks[i])/2)\n",
    "                end_invalid=int((self.peaks[i]+self.peaks[i+1])/2)\n",
    "                count=count+len(self.l1_filter[start_invalid:end_invalid])\n",
    "\n",
    "        invalid_data=round(count/len(self.l1_filter)*100,2)\n",
    "        print(\"Percentage of invalid data : \",invalid_data, \"%\")\n",
    "        \n",
    "    #####################################################################################################        \n",
    "     \n",
    "    def plot_Peaks(self):\n",
    "        \n",
    "        posValid=[i for i, e in enumerate(self.peaksValid) if e == 1]\n",
    "        \n",
    "        if len(posValid)==0 :\n",
    "            plt.plot(self.l1_filter)\n",
    "            plt.show()  \n",
    "            return \n",
    "        \n",
    "        normal_peaks=self.peaks[posValid]\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(self.l1_filter)\n",
    "        plt.plot(self.peaks, self.l1_filter[self.peaks], \"x\")\n",
    "        plt.plot(normal_peaks, self.l1_filter[normal_peaks], \"o\")\n",
    "        plt.show()  \n",
    "    \n",
    "    #####################################################################################################    \n",
    "\n",
    "    def ECG_stats(self):\n",
    "        IBI, HR, HRV, tmpHRV = [], [], [], []\n",
    "        windowHRV=0\n",
    "        \n",
    "        peaks50 = [item for item in self.peaks if item > self.idx]\n",
    "        peaksValid50=self.peaksValid[len(self.peaks)-len(peaks50):]\n",
    "        \n",
    "        if sum(peaksValid50) == 0 : \n",
    "            \n",
    "            dict= {\n",
    "            \"ECG_vs1\":\"nan\",\n",
    "            \"ECG_vs2\":\"nan\",\n",
    "            \"ECG_vs3\":\"nan\",\n",
    "            \"ECG_vs4\":\"nan\",\n",
    "                                         \n",
    "            \"ECG_vs_max\":\"nan\",\n",
    "            \"ECG_vs_min\":\"nan\",\n",
    "            \"ECG_vs_area\":\"nan\",\n",
    "            \"ECG_vs_std\":\"nan\",\n",
    "            \n",
    "            \"ECG_s1\":\"nan\",\n",
    "            \"ECG_s2\":\"nan\",\n",
    "            \"ECG_s3\":\"nan\",\n",
    "            \"ECG_s4\":\"nan\",\n",
    "            \"ECG_s5\":\"nan\",\n",
    "            \"ECG_s6\":\"nan\",\n",
    "            \"ECG_s7\":\"nan\",\n",
    "            \"ECG_s8\":\"nan\",\n",
    "            \"ECG_s9\":\"nan\",\n",
    "            \"ECG_s10\":\"nan\",\n",
    "            \n",
    "            \"ECG_h1\":\"nan\",\n",
    "            \"ECG_h2\":\"nan\",\n",
    "            \"ECG_h3\":\"nan\",\n",
    "            \"ECG_h4\":\"nan\",\n",
    "            \"ECG_h5\":\"nan\",\n",
    "            \"ECG_h6\":\"nan\",\n",
    "            \"ECG_h7\":\"nan\",\n",
    "            \"ECG_h8\":\"nan\",\n",
    "            \"ECG_h9\":\"nan\",\n",
    "            \"ECG_h10\":\"nan\",\n",
    "            \n",
    "            \"ECG_IBI_mean\":\"nan\", \n",
    "            \"ECG_IBI_std\":\"nan\",\n",
    "            \"ECG_IBI_skw\":\"nan\", \n",
    "            \"ECG_IBI_krt\":\"nan\",  \n",
    "            \"ECG_IBI_PERQ1\":\"nan\",\n",
    "            \"ECG_IBI_PERQ3\":\"nan\",\n",
    "            \"ECG_IBI_MIN\":\"nan\",\n",
    "            \"ECG_IBI_MAX\":\"nan\",\n",
    "            \n",
    "            \"ECG_HR_mean\":\"nan\", \n",
    "            \"ECG_HR_std\":\"nan\",\n",
    "            \"ECG_HR_skw\":\"nan\", \n",
    "            \"ECG_HR_krt\":\"nan\",  \n",
    "            \"ECG_HR_PERQ1\":\"nan\",\n",
    "            \"ECG_HR_PERQ3\":\"nan\",\n",
    "            \"ECG_HR_MIN\":\"nan\",\n",
    "            \"ECG_HR_MAX\":\"nan\",\n",
    "            \n",
    "            \"ECG_HRV_mean\":\"nan\", \n",
    "            \"ECG_HRV_std\":\"nan\",\n",
    "            \"ECG_HRV_skw\":\"nan\", \n",
    "            \"ECG_HRV_krt\":\"nan\",  \n",
    "            \"ECG_HRV_PERQ1\":\"nan\",\n",
    "            \"ECG_HRV_PERQ3\":\"nan\",\n",
    "            \"ECG_HRV_MIN\":\"nan\",\n",
    "            \"ECG_HRV_MAX\":\"nan\",\n",
    "        }\n",
    "            return dict\n",
    "        \n",
    "        for i in range (1, len(peaks50)-1) :\n",
    "            \n",
    "            if peaksValid50[i]==1 and peaksValid50[i-1]==1:\n",
    "                \n",
    "                interval=self.time[peaks50[i]]- self.time[peaks50[i-1]]\n",
    "                IBI.append(interval)\n",
    "                HR.append(60000/interval) #Standard for regular heart beats\n",
    "                \n",
    "                tmpHRV.append(interval)\n",
    "                windowHRV=windowHRV+tmpHRV[len(tmpHRV)-1]\n",
    "                \n",
    "            if (peaksValid50[i]==0) : #correction due to non-consecutives peaks\n",
    "                \n",
    "                windowHRV=0\n",
    "                tmpHRV = []\n",
    "                \n",
    "            if (windowHRV>5000) :\n",
    "                \n",
    "                RMSSD=0\n",
    "                \n",
    "                for j in range(0,len(tmpHRV)-2) :\n",
    "                    RMSSD=RMSSD+pow(tmpHRV[j]-tmpHRV[j+1],2)\n",
    "                \n",
    "                HRV.append(math.sqrt(RMSSD/(len(tmpHRV)-2)))\n",
    "                windowHRV=windowHRV-tmpHRV[0]\n",
    "                tmpHRV.pop(0)\n",
    "        \n",
    "        IBI_stats=stats.describe(IBI) \n",
    "        IBI_mean, IBI_std = IBI_stats[2], math.sqrt(IBI_stats[3])        \n",
    "        \n",
    "        HR_stats=stats.describe(HR) \n",
    "        HR_mean, HR_std = HR_stats[2], math.sqrt(HR_stats[3])\n",
    "        \n",
    "        if len(HRV) == 0 : \n",
    "            HRV_stats=[\"nan\",[\"nan\",\"nan\"],\"nan\",\"nan\",\"nan\",\"nan\"]\n",
    "            HRV_mean, HRV_std = \"nan\",\"nan\"\n",
    "            HRV_PERQ1, HRV_PERQ3 =\"nan\",\"nan\"\n",
    "        else :\n",
    "            HRV_stats=stats.describe(HRV) \n",
    "            HRV_mean, HRV_std = HRV_stats[2], math.sqrt(HRV_stats[3])\n",
    "            HRV_PERQ1, HRV_PERQ3 = percentQ1Q3(HRV, HRV_mean, HRV_std)\n",
    "        \n",
    "        IBI_PERQ1, IBI_PERQ3 = percentQ1Q3(IBI, IBI_mean, IBI_std)\n",
    "        HR_PERQ1, HR_PERQ3 = percentQ1Q3(HR, HR_mean, HR_std)\n",
    "        \n",
    "        # 4 very slow responses\n",
    "        num=4\n",
    "        band = [0,0.04]\n",
    "        vs=[]\n",
    "        subSTART=band[0]\n",
    "        for i in range (0,4) : \n",
    "            subEND=subSTART+0.025\n",
    "            vs.append(bandpower(np.array(self.l50), self.fs, [subSTART,subEND], nfft=len((self.l50))))\n",
    "            subSTART=subSTART+0.005\n",
    "        \n",
    "        \n",
    "        # 10 slow responses\n",
    "        num=10\n",
    "        band = [0,2.4]\n",
    "        s= []\n",
    "        subSTART=band[0]\n",
    "        for i in range (0,10) : \n",
    "            subEND=subSTART+band[1]/num\n",
    "            s.append(bandpower(np.array(self.l50), self.fs, [subSTART,subEND]))\n",
    "            subSTART=subEND\n",
    "        \n",
    "        \n",
    "        # 10 high responses\n",
    "        num=10\n",
    "        band = [0,100]\n",
    "        h= []\n",
    "        subSTART=band[0]\n",
    "        for i in range (0,10) : \n",
    "            subEND=subSTART+band[1]/num\n",
    "            h.append(bandpower(np.array(self.l50), self.fs, [subSTART,subEND]))\n",
    "            subSTART=subEND\n",
    "\n",
    "        vs_maxi, vs_mini, vs_std, vs_area = bandpower(np.array(self.l50), self.fs, [0,0.04], nfft=len(self.l50) , additional= True)\n",
    "\n",
    "        ECG_dict = {\n",
    "            \"ECG_vs1\":vs[0],\n",
    "            \"ECG_vs2\":vs[1],\n",
    "            \"ECG_vs3\":vs[2],\n",
    "            \"ECG_vs4\":vs[3],\n",
    "                                         \n",
    "            \"ECG_vs_max\":vs_maxi,\n",
    "            \"ECG_vs_min\":vs_mini,\n",
    "            \"ECG_vs_area\":vs_area,\n",
    "            \"ECG_vs_std\":vs_std,\n",
    "            \n",
    "            \"ECG_s1\":s[0],\n",
    "            \"ECG_s2\":s[1],\n",
    "            \"ECG_s3\":s[2],\n",
    "            \"ECG_s4\":s[3],\n",
    "            \"ECG_s5\":s[4],\n",
    "            \"ECG_s6\":s[5],\n",
    "            \"ECG_s7\":s[6],\n",
    "            \"ECG_s8\":s[7],\n",
    "            \"ECG_s9\":s[8],\n",
    "            \"ECG_s10\":s[9],\n",
    "            \n",
    "            \"ECG_h1\":h[0],\n",
    "            \"ECG_h2\":h[1],\n",
    "            \"ECG_h3\":h[2],\n",
    "            \"ECG_h4\":h[3],\n",
    "            \"ECG_h5\":h[4],\n",
    "            \"ECG_h6\":h[5],\n",
    "            \"ECG_h7\":h[6],\n",
    "            \"ECG_h8\":h[7],\n",
    "            \"ECG_h9\":h[8],\n",
    "            \"ECG_h10\":h[9],\n",
    "            \n",
    "            \"ECG_IBI_mean\":round(IBI_mean,3), \n",
    "            \"ECG_IBI_std\":round(IBI_std,3),\n",
    "            \"ECG_IBI_skw\":round(IBI_stats[4],3), \n",
    "            \"ECG_IBI_krt\":round(IBI_stats[5],3),  \n",
    "            \"ECG_IBI_PERQ1\":round(IBI_PERQ1,3),\n",
    "            \"ECG_IBI_PERQ3\":round(IBI_PERQ3,3),\n",
    "            \"ECG_IBI_MIN\":round(IBI_stats[1][0],3),\n",
    "            \"ECG_IBI_MAX\":round(IBI_stats[1][1],3),\n",
    "            \n",
    "            \"ECG_HR_mean\":round(HR_mean,3), \n",
    "            \"ECG_HR_std\":round(HR_std,3),\n",
    "            \"ECG_HR_skw\":round(HR_stats[4],3), \n",
    "            \"ECG_HR_krt\":round(HR_stats[5],3),  \n",
    "            \"ECG_HR_PERQ1\":round(HR_PERQ1,3),\n",
    "            \"ECG_HR_PERQ3\":round(HR_PERQ3,3),\n",
    "            \"ECG_HR_MIN\":round(HR_stats[1][0],3),\n",
    "            \"ECG_HR_MAX\":round(HR_stats[1][1],3),\n",
    "            \n",
    "            \"ECG_HRV_mean\":HRV_mean, \n",
    "            \"ECG_HRV_std\":HRV_std,\n",
    "            \"ECG_HRV_skw\":HRV_stats[4], \n",
    "            \"ECG_HRV_krt\":HRV_stats[5],  \n",
    "            \"ECG_HRV_PERQ1\":HRV_PERQ1,\n",
    "            \"ECG_HRV_PERQ3\":HRV_PERQ3,\n",
    "            \"ECG_HRV_MIN\":HRV_stats[1][0],\n",
    "            \"ECG_HRV_MAX\":HRV_stats[1][1],\n",
    "        }\n",
    "        \n",
    "        \n",
    "        return ECG_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1\n",
    "\n",
    "Plot the Power Spectral Density (PSD) for the Lead I ECG signal of the first participant watching Clip 1 from 0–40 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_1_1=ECG(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe362763e92482bb24d7105b7227161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "ECG_1_1.plot_PSD(band=[0,40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2\n",
    "\n",
    "*Propose and reason about, implement and apply a suitable low-pass filter to remove noise. Plot the filter’s frequency response. Plot the filtered ECG signal in the time-domain.*\n",
    "\n",
    "According to [2], common frequencies of the important components on the ECG:\n",
    "\n",
    "- Heart rate: 0.67 – 5 Hz (i.e. 40 – 300 bpm)\n",
    "- P-wave: 0.67 – 5 Hz\n",
    "- QRS: 10 – 50 Hz\n",
    "- T-wave: 1 – 7 Hz\n",
    "- High frequency potentials: 100 – 500 Hz\n",
    "\n",
    "The common frequencies of the artifact and noise on the ECG:\n",
    "\n",
    "- Muscle: 5 – 50 Hz\n",
    "- Respiratory: 0.12 – 0.5 Hz (e.g. 8 – 30 bpm)\n",
    "- External electrical: 50 or 60 Hz (A/C mains or line frequency)\n",
    "- Other electrical: typically >10 Hz (muscle stimulators, strong magnetic fields, pacemakers with impedance monitoring)\n",
    "\n",
    "The skin-electrode interface requires special note, as it is the largest source of interference, producing a DC component of 200-300 mV. Compare this to the electrical activity of your heart, which is in the range of 0.1 to 2 mV! The interference seen from this component is magnified by motion, either patient movement, or respiratory variation.\n",
    "\n",
    "Low-pass filters on the ECG are used to remove high frequency muscle artifact and external interference. They typically attenuate only the amplitude of higher frequency ECG components. Analog low-pass filtering has a noticeable affect on the QRS complex, epsilon, and J-waves but do not alter repolarization signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f018135e2814f3ca2e5ccc7a02b9e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "ECG_1_1.plot_ECG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3\n",
    "\n",
    "*Using the algorithm for artifact detection introduced in Lecture 6 or another suitable approach, compute the percentage of ECG data that is flagged as having artifacts. Select one such ECG signal that contains artifacts and plot it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of invalid data :  3.31 %\n"
     ]
    }
   ],
   "source": [
    "ECG_1_1.invalidData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107a5fd5a96640eabc4e04f3788a57aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "ECG_1_1.plot_Peaks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 \n",
    "\n",
    "*For the last 50 seconds of each recording, extract the following features as mentioned in the paper\n",
    "#01 ten low frequency ([0–2.4] Hz) PSDs\n",
    "#02 four very slow response ([0–0.04] Hz) PSDs\n",
    "#03 statistical measurements over inter beat intervals\n",
    "#04 statistical measurements over heart rate\n",
    "#05 statistical measurements over heart rate variability *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention used to collect features names\n",
    "features=[list(ECG_1_1.ECG_stats().keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ECG_vs1': 155450.379,\n",
       " 'ECG_vs2': 0.0,\n",
       " 'ECG_vs3': 0.0,\n",
       " 'ECG_vs4': 623053.455,\n",
       " 'ECG_vs_max': 46760345.749918,\n",
       " 'ECG_vs_min': 38.146972656835324,\n",
       " 'ECG_vs_area': 726269.219,\n",
       " 'ECG_vs_std': 23813756.613865122,\n",
       " 'ECG_s1': 4864870.465,\n",
       " 'ECG_s2': 1665.935,\n",
       " 'ECG_s3': 32.101,\n",
       " 'ECG_s4': 6.764,\n",
       " 'ECG_s5': 1.212,\n",
       " 'ECG_s6': 0.436,\n",
       " 'ECG_s7': 0.14,\n",
       " 'ECG_s8': 0.04,\n",
       " 'ECG_s9': 0.029,\n",
       " 'ECG_s10': 0.014,\n",
       " 'ECG_h1': 5860006.459,\n",
       " 'ECG_h2': 0.0,\n",
       " 'ECG_h3': 0.0,\n",
       " 'ECG_h4': 0.0,\n",
       " 'ECG_h5': 0.0,\n",
       " 'ECG_h6': 0.0,\n",
       " 'ECG_h7': 0.0,\n",
       " 'ECG_h8': 0.0,\n",
       " 'ECG_h9': 0.0,\n",
       " 'ECG_h10': 0.0,\n",
       " 'ECG_IBI_mean': 891.001,\n",
       " 'ECG_IBI_std': 31.348,\n",
       " 'ECG_IBI_skw': 0.029,\n",
       " 'ECG_IBI_krt': -0.197,\n",
       " 'ECG_IBI_PERQ1': 0.173,\n",
       " 'ECG_IBI_PERQ3': 0.154,\n",
       " 'ECG_IBI_MIN': 812.5,\n",
       " 'ECG_IBI_MAX': 957.031,\n",
       " 'ECG_HR_mean': 67.422,\n",
       " 'ECG_HR_std': 2.378,\n",
       " 'ECG_HR_skw': 0.164,\n",
       " 'ECG_HR_krt': -0.064,\n",
       " 'ECG_HR_PERQ1': 0.154,\n",
       " 'ECG_HR_PERQ3': 0.173,\n",
       " 'ECG_HR_MIN': 62.694,\n",
       " 'ECG_HR_MAX': 73.846,\n",
       " 'ECG_HRV_mean': 30.65323650175001,\n",
       " 'ECG_HRV_std': 9.230421297958364,\n",
       " 'ECG_HRV_skw': 0.02671217204114921,\n",
       " 'ECG_HRV_krt': -1.061410872749768,\n",
       " 'ECG_HRV_PERQ1': 0.238,\n",
       " 'ECG_HRV_PERQ3': 0.143,\n",
       " 'ECG_HRV_MIN': 15.746597164645605,\n",
       " 'ECG_HRV_MAX': 47.961051397690134}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ECG_1_1.ECG_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 \n",
    "\n",
    "Statistical measurements (band power area, standard deviation, min and max) of the entire 0-0.04Hz band were also collected as additional features, because the integrated power of the four sub-bands in the same frequency domain was every so often unresponsive. The PSD frequencies band 0-100Hz was divided in 10 sub bands and their band power was integrated to collect additional features. These features were said to contain information about emotion recognition in a previous work (see report). \n",
    "As a personal curiosity, the maximum and minimum measurement in HR, HRV and IBI was extracted to study to what extent the greatest/smallest HR/HRV/IBI can have a role to predict emotional state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_Features :\n",
    "    \n",
    "    def __init__(self, participant, clip):\n",
    "        \n",
    "        participant_data = data[participant-1]\n",
    "\n",
    "        self.EEGF=participant_data[\"recordings\"][clip][\"EEG_features\"]\n",
    "        \n",
    "        for i in range (0,len(self.EEGF)):\n",
    "        \n",
    "            if math.isnan(self.EEGF[i]) :\n",
    "                \n",
    "                self.EEGF[i]=0\n",
    "    \n",
    "    \n",
    "    def get_x (self):\n",
    "        \n",
    "        return self.EEGF\n",
    "    \n",
    "    \n",
    "\n",
    "def get_keys() :\n",
    "    \n",
    "    feature_list=[]\n",
    "    \n",
    "    for i in range (1, 89) :\n",
    "        \n",
    "        key=\"EEG_\"+str(i)\n",
    "        \n",
    "        feature_list.append(key)        \n",
    "        \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my=get_keys()\n",
    "features=[*features[0], *my] # \"features\" is declared in ECG Data -> Answers -> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.19802936e+02,  7.77842733e+01,  2.26021933e-01,  6.68486240e+00,\n",
       "        9.74391006e-02,  1.07432854e-01, -9.16415405e-02,  5.35000000e-01,\n",
       "        7.59525297e-01,  1.37564465e+00,  7.79512804e-01,  4.36164897e+02,\n",
       "        1.20961906e+02, -2.30458070e-01,  1.88955345e+00,  2.14865709e-01,\n",
       "        2.08619613e-01, -1.77698746e-01,  6.03125000e-01,  1.19925047e-01,\n",
       "        1.28468001e+00,  1.39912555e-01,  6.24212680e+02,  1.52448509e+02,\n",
       "       -3.71841108e-01,  2.84268586e+00,  1.47407870e-01,  1.41786384e-01,\n",
       "        3.28766632e-02,  5.65625000e-01,  1.19925047e-01, -6.50878954e-02,\n",
       "        1.39912555e-01,  2.48899126e+02,  9.49282376e+01,  9.05412382e-01,\n",
       "        3.64877049e+00,  1.34291068e-01,  1.64272330e-01,  6.25640106e-02,\n",
       "        5.75000000e-01,  1.19925047e-01, -4.86872624e-01,  1.39912555e-01,\n",
       "        1.35783260e+02,  5.17711719e+01,  5.41444361e-01,  2.85951611e+00,\n",
       "        1.36789507e-01,  1.88632105e-01, -2.54845428e-02,  5.91875000e-01,\n",
       "        1.19925047e-01,  1.20435518e+00,  1.39912555e-01,  4.46861337e+01,\n",
       "        1.58715258e+01,  5.75177898e-01,  2.26832963e+00,  1.80512180e-01,\n",
       "        1.84259838e-01, -6.54716492e-03,  6.70000000e-01,  1.19925047e-01,\n",
       "        1.19390870e+00,  1.19925047e-01,  5.03757027e+01,  1.71293473e+01,\n",
       "        4.80683273e-01,  2.54510398e+00,  1.98625859e-01,  1.83010618e-01,\n",
       "       -1.82116699e-02,  6.14375000e-01,  7.99500312e-02,  5.26900274e+00,\n",
       "        7.99500312e-02,  8.73335415e+01,  5.11165298e+01,  9.07230959e-01,\n",
       "        3.06240256e+00,  1.54903186e-01,  1.52404747e-01,  1.55822754e-03,\n",
       "        6.41250000e-01,  7.99500312e-02, -2.02390648e-01,  9.99375390e-02])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEG_Features(1,1).get_x()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMO:   \n",
    "    \n",
    "    def __init__(self, participant, clip):\n",
    "        \n",
    "        participant_data = data[participant-1]\n",
    "\n",
    "        self.nEMO=participant_data[\"recordings\"][clip]['EMO']\n",
    "         \n",
    "    #####################################################################################################    \n",
    "\n",
    "    def EMO_stats(self):\n",
    "        emoStats = []\n",
    "        keys=['vdUL', 'vdLL', 'hdLLC', 'vdLLC', 'hdRLC', 'vdRLC', 'dRE', 'dLE', 'dRC', 'dLC', 'dRL', 'dLL']\n",
    "        for i in range(1,13):\n",
    "            \n",
    "            aStat = []\n",
    "            for j in range (0,len(self.nEMO)):\n",
    "                if math.isnan(self.nEMO[j][i]):\n",
    "                    continue\n",
    "                else : \n",
    "                    aStat.append(self.nEMO[j][i]) \n",
    "            \n",
    "            \n",
    "            aStat_d=stats.describe(aStat) \n",
    "            aStat_d_mean, aStat_d_std = aStat_d[2], math.sqrt(aStat_d[3]) \n",
    "            aStat_d_PERQ1, aStat_d_PERQ3 = percentQ1Q3(aStat, aStat_d_mean, aStat_d_std)\n",
    "            a_dict = {\n",
    "                \"EMO_\"+keys[i-1]+\"_mean\":round(aStat_d_mean,3), \n",
    "                \"EMO_\"+keys[i-1]+\"_std\":round(aStat_d_std,3),\n",
    "                \"EMO_\"+keys[i-1]+\"_skw\":round(aStat_d[4],3), \n",
    "                \"EMO_\"+keys[i-1]+\"_krt\":round(aStat_d[5],3),  \n",
    "                \"EMO_\"+keys[i-1]+\"_PERQ1\":round(aStat_d_PERQ1,3),\n",
    "                \"EMO_\"+keys[i-1]+\"_PERQ3\":round(aStat_d_PERQ3,3),\n",
    "            }\n",
    "            emoStats.append(a_dict)\n",
    "                  \n",
    "        emo= {**emoStats[0], **emoStats[1], **emoStats[2], **emoStats[3], **emoStats[4], **emoStats[5], **emoStats[6], **emoStats[7], **emoStats[8], **emoStats[9], **emoStats[10], **emoStats[11]}\n",
    "        \n",
    "        return emo\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1R1_EMO=EMO(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my=P1R1_EMO.EMO_stats().keys()\n",
    "features=[*features, *my] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMO_vdUL_mean': 0.077,\n",
       " 'EMO_vdUL_std': 0.136,\n",
       " 'EMO_vdUL_skw': 0.066,\n",
       " 'EMO_vdUL_krt': 0.609,\n",
       " 'EMO_vdUL_PERQ1': 0.138,\n",
       " 'EMO_vdUL_PERQ3': 0.148,\n",
       " 'EMO_vdLL_mean': -0.052,\n",
       " 'EMO_vdLL_std': 0.037,\n",
       " 'EMO_vdLL_skw': -0.327,\n",
       " 'EMO_vdLL_krt': -0.075,\n",
       " 'EMO_vdLL_PERQ1': 0.162,\n",
       " 'EMO_vdLL_PERQ3': 0.143,\n",
       " 'EMO_hdLLC_mean': -0.054,\n",
       " 'EMO_hdLLC_std': 0.081,\n",
       " 'EMO_hdLLC_skw': -0.276,\n",
       " 'EMO_hdLLC_krt': 0.225,\n",
       " 'EMO_hdLLC_PERQ1': 0.16,\n",
       " 'EMO_hdLLC_PERQ3': 0.184,\n",
       " 'EMO_vdLLC_mean': 0.262,\n",
       " 'EMO_vdLLC_std': 0.186,\n",
       " 'EMO_vdLLC_skw': 1.127,\n",
       " 'EMO_vdLLC_krt': 1.551,\n",
       " 'EMO_vdLLC_PERQ1': 0.075,\n",
       " 'EMO_vdLLC_PERQ3': 0.141,\n",
       " 'EMO_hdRLC_mean': 0.244,\n",
       " 'EMO_hdRLC_std': 0.06,\n",
       " 'EMO_hdRLC_skw': 0.344,\n",
       " 'EMO_hdRLC_krt': 0.459,\n",
       " 'EMO_hdRLC_PERQ1': 0.138,\n",
       " 'EMO_hdRLC_PERQ3': 0.161,\n",
       " 'EMO_vdRLC_mean': 0.343,\n",
       " 'EMO_vdRLC_std': 0.237,\n",
       " 'EMO_vdRLC_skw': 0.915,\n",
       " 'EMO_vdRLC_krt': 0.687,\n",
       " 'EMO_vdRLC_PERQ1': 0.171,\n",
       " 'EMO_vdRLC_PERQ3': 0.128,\n",
       " 'EMO_dRE_mean': 0.076,\n",
       " 'EMO_dRE_std': 0.083,\n",
       " 'EMO_dRE_skw': 2.433,\n",
       " 'EMO_dRE_krt': 15.221,\n",
       " 'EMO_dRE_PERQ1': 0.084,\n",
       " 'EMO_dRE_PERQ3': 0.145,\n",
       " 'EMO_dLE_mean': 0.013,\n",
       " 'EMO_dLE_std': 0.068,\n",
       " 'EMO_dLE_skw': 3.784,\n",
       " 'EMO_dLE_krt': 29.831,\n",
       " 'EMO_dLE_PERQ1': 0.069,\n",
       " 'EMO_dLE_PERQ3': 0.115,\n",
       " 'EMO_dRC_mean': -0.142,\n",
       " 'EMO_dRC_std': 0.199,\n",
       " 'EMO_dRC_skw': 0.067,\n",
       " 'EMO_dRC_krt': 0.682,\n",
       " 'EMO_dRC_PERQ1': 0.144,\n",
       " 'EMO_dRC_PERQ3': 0.127,\n",
       " 'EMO_dLC_mean': 0.27,\n",
       " 'EMO_dLC_std': 0.438,\n",
       " 'EMO_dLC_skw': 0.123,\n",
       " 'EMO_dLC_krt': -1.092,\n",
       " 'EMO_dLC_PERQ1': 0.213,\n",
       " 'EMO_dLC_PERQ3': 0.191,\n",
       " 'EMO_dRL_mean': 0.655,\n",
       " 'EMO_dRL_std': 0.411,\n",
       " 'EMO_dRL_skw': 1.674,\n",
       " 'EMO_dRL_krt': 2.738,\n",
       " 'EMO_dRL_PERQ1': 0.084,\n",
       " 'EMO_dRL_PERQ3': 0.125,\n",
       " 'EMO_dLL_mean': 0.298,\n",
       " 'EMO_dLL_std': 0.227,\n",
       " 'EMO_dLL_skw': 2.099,\n",
       " 'EMO_dLL_krt': 4.805,\n",
       " 'EMO_dLL_PERQ1': 0.035,\n",
       " 'EMO_dLL_PERQ3': 0.114}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1R1_EMO.EMO_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA/GSM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEDA(data, clip):\n",
    "    \"\"\"Load GSR from the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "      dictionary of the participant.\n",
    "    clip : int\n",
    "      number of clip\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    time : 1d-array\n",
    "      The time series for the given clip when data is available.\n",
    "    l1 : 1d-array\n",
    "      The GSR series for the given clip when data is available.\n",
    "    fs : float\n",
    "      Sampling frequency.      \n",
    "    \"\"\"\n",
    "        \n",
    "    nEDA=data[\"recordings\"][clip]['GSR']\n",
    "    \n",
    "    EDA, time = [], []\n",
    "    \n",
    "    for i in range (0,len(nEDA)-1):\n",
    "        if math.isnan(nEDA[i][1]) or math.isnan(nEDA[i][0]) :\n",
    "            continue\n",
    "        else : \n",
    "            EDA.append(nEDA[i][1])\n",
    "            time.append(nEDA[i][0])\n",
    "        \n",
    "    fs=data[\"FS_GSR\"]\n",
    "    \n",
    "    return time, EDA, fs\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order):\n",
    "    \"\"\"Low pass filter used\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1d-array\n",
    "      Input signal\n",
    "    cutoff : float\n",
    "      Cutoff frequency\n",
    "    fs : float\n",
    "      Sampling frequency\n",
    "    order : int\n",
    "      Order of the filter\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    y : 1d-array\n",
    "      Filtered signal\n",
    "    \"\"\"  \n",
    "    nyq = 0.5 * fs\n",
    "    \n",
    "    normal_cutoff = cutoff / nyq\n",
    "    \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    \n",
    "    y = filtfilt(b, a, data)\n",
    "    \n",
    "    return y\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "def cvxEDA(y, delta, tau0=2., tau1=0.7, delta_knot=10., alpha=8e-4, gamma=1e-2,\n",
    "           solver=None, options={'reltol':1e-9}):\n",
    "    \"\"\"\n",
    "    ______________________________________________________________________________\n",
    "    Copyright (C) 2014-2015 Luca Citi, Alberto Greco\n",
    "    ______________________________________________________________________________\n",
    "     This method was first proposed in:\n",
    "     A Greco, G Valenza, A Lanata, EP Scilingo, and L Citi\n",
    "     \"cvxEDA: a Convex Optimization Approach to Electrodermal Activity Processing\"\n",
    "     IEEE Transactions on Biomedical Engineering, 2015\n",
    "     DOI: 10.1109/TBME.2015.2474131\n",
    "     ______________________________________________________________________________\n",
    "    \n",
    "    CVXEDA Convex optimization approach to electrodermal activity processing\n",
    "    Arguments:\n",
    "       y: observed EDA signal (we recommend normalizing it: y = zscore(y))\n",
    "       delta: sampling interval (in seconds) of y\n",
    "       tau0: slow time constant of the Bateman function\n",
    "       tau1: fast time constant of the Bateman function\n",
    "       delta_knot: time between knots of the tonic spline function\n",
    "       alpha: penalization for the sparse SMNA driver\n",
    "       gamma: penalization for the tonic spline coefficients\n",
    "       solver: sparse QP solver to be used, see cvxopt.solvers.qp\n",
    "       options: solver options, see: http://cvxopt.org/userguide/coneprog.html#algorithm-parameters\n",
    "    Returns (see paper for details):\n",
    "       r: phasic component\n",
    "       p: sparse SMNA driver of phasic component\n",
    "       t: tonic component\n",
    "       l: coefficients of tonic spline\n",
    "       d: offset and slope of the linear drift term\n",
    "       e: model residuals\n",
    "       obj: value of objective function being minimized (eq 15 of paper)\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(y)\n",
    "    y = cv.matrix(y)\n",
    "\n",
    "    # bateman ARMA model\n",
    "    a1 = 1./min(tau1, tau0) # a1 > a0\n",
    "    a0 = 1./max(tau1, tau0)\n",
    "    ar = np.array([(a1*delta + 2.) * (a0*delta + 2.), 2.*a1*a0*delta**2 - 8.,\n",
    "        (a1*delta - 2.) * (a0*delta - 2.)]) / ((a1 - a0) * delta**2)\n",
    "    ma = np.array([1., 2., 1.])\n",
    "\n",
    "    # matrices for ARMA model\n",
    "    i = np.arange(2, n)\n",
    "    A = cv.spmatrix(np.tile(ar, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n",
    "    M = cv.spmatrix(np.tile(ma, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n",
    "\n",
    "    # spline\n",
    "    delta_knot_s = int(round(delta_knot / delta))\n",
    "    spl = np.r_[np.arange(1.,delta_knot_s), np.arange(delta_knot_s, 0., -1.)] # order 1\n",
    "    spl = np.convolve(spl, spl, 'full')\n",
    "    spl /= max(spl)\n",
    "    \n",
    "    # matrix of spline regressors\n",
    "    i = np.c_[np.arange(-(len(spl)//2), (len(spl)+1)//2)] + np.r_[np.arange(0, n, delta_knot_s)]\n",
    "    nB = i.shape[1]\n",
    "    j = np.tile(np.arange(nB), (len(spl),1))\n",
    "    p = np.tile(spl, (nB,1)).T\n",
    "    valid = (i >= 0) & (i < n)\n",
    "    B = cv.spmatrix(p[valid], i[valid], j[valid])\n",
    "\n",
    "    # trend\n",
    "    C = cv.matrix(np.c_[np.ones(n), np.arange(1., n+1.)/n])\n",
    "    nC = C.size[1]\n",
    "\n",
    "    # Solve the problem:\n",
    "    # .5*(M*q + B*l + C*d - y)^2 + alpha*sum(A,1)*p + .5*gamma*l'*l\n",
    "    # s.t. A*q >= 0\n",
    "\n",
    "    old_options = cv.solvers.options.copy()\n",
    "    cv.solvers.options.clear()\n",
    "    cv.solvers.options.update(options)\n",
    "    if solver == 'conelp':\n",
    "        # Use conelp\n",
    "        z = lambda m,n: cv.spmatrix([],[],[],(m,n))\n",
    "        G = cv.sparse([[-A,z(2,n),M,z(nB+2,n)],[z(n+2,nC),C,z(nB+2,nC)],\n",
    "                    [z(n,1),-1,1,z(n+nB+2,1)],[z(2*n+2,1),-1,1,z(nB,1)],\n",
    "                    [z(n+2,nB),B,z(2,nB),cv.spmatrix(1.0, range(nB), range(nB))]])\n",
    "        h = cv.matrix([z(n,1),.5,.5,y,.5,.5,z(nB,1)])\n",
    "        c = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T,z(nC,1),1,gamma,z(nB,1)])\n",
    "        res = cv.solvers.conelp(c, G, h, dims={'l':n,'q':[n+2,nB+2],'s':[]})\n",
    "        obj = res['primal objective']\n",
    "    else:\n",
    "        # Use qp\n",
    "        Mt, Ct, Bt = M.T, C.T, B.T\n",
    "        H = cv.sparse([[Mt*M, Ct*M, Bt*M], [Mt*C, Ct*C, Bt*C], \n",
    "                    [Mt*B, Ct*B, Bt*B+gamma*cv.spmatrix(1.0, range(nB), range(nB))]])\n",
    "        f = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T - Mt*y,  -(Ct*y), -(Bt*y)])\n",
    "        res = cv.solvers.qp(H, f, cv.spmatrix(-A.V, A.I, A.J, (n,len(f))),\n",
    "                            cv.matrix(0., (n,1)), solver=solver)\n",
    "        obj = res['primal objective'] + .5 * (y.T * y)\n",
    "    cv.solvers.options.clear()\n",
    "    cv.solvers.options.update(old_options)\n",
    "\n",
    "    l = res['x'][-nB:]\n",
    "    d = res['x'][n:n+nC]\n",
    "    t = B*l + C*d\n",
    "    q = res['x'][:n]\n",
    "    p = A * q\n",
    "    r = M * q\n",
    "    e = y - r - t\n",
    "\n",
    "    return (np.array(a).ravel() for a in (r, p, t, l, d, e, obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA:\n",
    "\n",
    "    def __init__(self, participant, clip):\n",
    "        \n",
    "        participant_data = data[participant-1]\n",
    "        \n",
    "        self.time, self.EDA, self.fs = loadEDA(participant_data, clip)\n",
    "        \n",
    "        self.NormEDA=((self.EDA - statistics.mean(self.EDA) )/ statistics.stdev(self.EDA))\n",
    "\n",
    "        self.gsr=butter_lowpass_filter(self.NormEDA, 5, self.fs, 1) #cutoff , order\n",
    "        \n",
    "        self.f, self.Pxx_den = signal.welch(self.gsr, self.fs, nperseg=self.fs*15, noverlap=self.fs*10)\n",
    "        \n",
    "        self.gsr50, self.time50, self.idx = last50Sec(self.time, self.gsr)\n",
    "        \n",
    "        \n",
    "    #####################################################################################################           \n",
    "    \n",
    "    def plot_PSD(self):\n",
    "        \n",
    "        f, Pxx_den = signal.welch(self.gsr, self.fs, nperseg=self.fs*15, noverlap=self.fs*10)\n",
    "        \n",
    "        plt.plot(self.f, self.Pxx_den, 'b-', label='filtered')\n",
    "        #plt.plot(f, Pxx_den, 'g-', label='non-filtered')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.xlabel('Frequencies')\n",
    "        plt.ylabel('PSD')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    #####################################################################################################        \n",
    "    \n",
    "    def plot_filter(self):\n",
    "        \n",
    "        plt.plot(self.time, self.gsr, 'b-', label='filtered')\n",
    "        #plt.plot(self.time, self.GSR50, 'g-', label='non-filtered')\n",
    "        #plt.xlim([0,10])\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Hz')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_time(self):\n",
    "                \n",
    "        [r, p, t, l, d, e, obj] = cvxEDA(self.gsr, 1/self.fs)\n",
    "                \n",
    "        plt.plot(self.time, self.cond, 'b-', label='Normalized-Filtered EDA')\n",
    "        #plt.plot(self.time, r, 'g-', label='Phasic SC Response')\n",
    "        #plt.plot(self.time, p, 'y-', label='sparse SMNA driver of phasic component')\n",
    "        #plt.plot(self.time, t, 'r-', label='Tonic SC Level')\n",
    "        #plt.plot(self.time, self.y_spl[0], 'o-', label='1st Dev')\n",
    "        \n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('B')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    #####################################################################################################        \n",
    "    # resistance = self.r \n",
    "    \n",
    "    def EDA_stats(self):\n",
    "        \n",
    "        self.cond50= np.array([1]*len(self.gsr50))/np.array(self.gsr50)\n",
    "        \n",
    "        # resistance 1st derivatives\n",
    "        r_1dev=np.gradient(self.gsr50)/np.gradient(self.time50)\n",
    "        r_1dev_neg = [item for item in r_1dev if item < 0]\n",
    "        \n",
    "        # conductance 1st derivatives\n",
    "        y1=np.gradient(self.cond50)\n",
    "        x1=np.gradient(self.time50)\n",
    "        c_1dev= np.divide(y1, x1, out=np.zeros_like(y1), where=x1!=0)\n",
    "        \n",
    "        # conductance 2nd derivatives\n",
    "        y2=np.gradient(y1)\n",
    "        x2=np.gradient(x1)\n",
    "        c_2dev= np.divide(y2, x2, out=np.zeros_like(y2), where=x2!=0)\n",
    "        \n",
    "        # 05\n",
    "        exp, neg= 0,0\n",
    "        for idx in range (0, len(r_1dev)-2):\n",
    "            exp= exp+ (self.time50[idx+1]-self.time50[idx])\n",
    "            if r_1dev[idx] < 0: \n",
    "                neg= neg+(self.time50[idx+1]-self.time50[idx])\n",
    "\n",
    "        # 08\n",
    "        rise = 0\n",
    "        for idx in range (0, len(self.gsr50)-2):\n",
    "            if (self.gsr50[idx+1] - self.gsr50[idx]) > 0: \n",
    "                rise = rise + (self.time50[idx+1]-self.time50[idx])\n",
    "        \n",
    "        \n",
    "        # 09 - 4 very slow responses\n",
    "        num=4\n",
    "        band = [0,0.04]\n",
    "        vs=[]\n",
    "        subSTART=band[0]\n",
    "        for i in range (0,num) : \n",
    "            subEND=subSTART+0.025\n",
    "            vs.append(bandpower(np.array(self.gsr50), self.fs, [subSTART,subEND], nfft=len(self.gsr50)))\n",
    "            subSTART=subSTART+0.005\n",
    "        \n",
    "        \n",
    "        # 15 - 10 slow responses\n",
    "        num=10\n",
    "        band = [0,2.4]\n",
    "        s= []\n",
    "        subSTART=band[0]\n",
    "        for i in range (0,num) : \n",
    "            subEND=subSTART+band[1]/num\n",
    "            s.append(bandpower(np.array(self.gsr50), self.fs, [subSTART,subEND]))\n",
    "            subSTART=subEND\n",
    "        \n",
    "        s_maxi, s_mini, s_std, s_area = bandpower(np.array(self.gsr50), self.fs, [0,2.4], nfft=len(self.gsr50) , additional= True)\n",
    "\n",
    "        vs_maxi, vs_mini, vs_std, vs_area = bandpower(np.array(self.gsr50), self.fs, [0,0.04], nfft=len(self.gsr50) , additional= True)\n",
    "        \n",
    "        EDA_dict = {\n",
    "            \"EDA_vs1\":vs[0],\n",
    "            \"EDA_vs2\":vs[1],\n",
    "            \"EDA_vs3\":vs[2],\n",
    "            \"EDA_vs4\":vs[3],\n",
    "            \n",
    "                        \n",
    "            \"EDA_vs_max\":vs_maxi,\n",
    "            \"EDA_vs_min\":vs_mini,\n",
    "            \"EDA_vs_area\":vs_area,\n",
    "            \"EDA_vs_std\":vs_std,\n",
    "            \n",
    "            \"EDA_s1\":s[0],\n",
    "            \"EDA_s2\":s[1],\n",
    "            \"EDA_s3\":s[2],\n",
    "            \"EDA_s4\":s[3],\n",
    "            \"EDA_s5\":s[4],\n",
    "            \"EDA_s6\":s[5],\n",
    "            \"EDA_s7\":s[6],\n",
    "            \"EDA_s8\":s[7],\n",
    "            \"EDA_s9\":s[8],\n",
    "            \"EDA_s10\":s[9],\n",
    "            \n",
    "            \"EDA_s_max\":s_maxi,\n",
    "            \"EDA_s_min\":s_mini,\n",
    "            \"EDA_s_area\":s_area,\n",
    "            \"EDA_s_std\":s_std,\n",
    "            \n",
    "            \"EDA_r_mean\": statistics.mean(self.gsr50), #01\n",
    "            \"EDA_r_mean_1dev\" : statistics.mean(r_1dev), #02\n",
    "            \"EDA_r_mean_1dev_abs\" : statistics.mean(abs(r_1dev)), #03\n",
    "            \"EDA_r_mean_1dev_neg\" : statistics.mean(r_1dev_neg), #04\n",
    "            \"EDA_r_mean_1dev_neg_perc\" : neg/exp, #05\n",
    "            \"EDA_r_std\":statistics.stdev(self.gsr50), #06\n",
    "            \"EDA_r_loc_min\":len(argrelextrema(np.array(self.gsr50), np.less)[0]), #14\n",
    "            \"EDA_avg_rise\" : rise/exp, #08\n",
    "            \n",
    "            \"EDA_c_loc_min\":len(argrelextrema(np.array(self.cond50), np.less)[0]), #07\n",
    "            \"EDA_c_std\":statistics.stdev(self.cond50), #10\n",
    "            \"EDA_c_mean_1dev\" : statistics.mean(c_1dev), #11\n",
    "            \"EDA_c_mean_1dev_abs\" : statistics.mean(abs(c_1dev)), #12\n",
    "            \"EDA_c_mean_2dev_abs\": statistics.mean(abs(c_2dev)), #13\n",
    "        }\n",
    "        \n",
    "        return EDA_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers to questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1R1_EDA=EDA(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my=P1R1_EDA.EDA_stats().keys()\n",
    "features=[*features, *my] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42032c8fafb492ab2ec003e6fd043aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "P1R1_EDA.plot_PSD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300133bb9c474257bdc32b62bed15906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "P1R1_EDA.plot_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3\n",
    "\n",
    "Galvanic skin resistance (GSR) \n",
    "- 08: average rising time of the GSR signal\n",
    "- 09: power density estimates; 4 sub-bands in the [0-0.4] Hz band\n",
    "- 15: log power density estimates; 10 sub-bands in the [0-2.4] Hz band\n",
    "- 01: mean skin resistance\n",
    "- 02: mean of first derivatives of skin resistance\n",
    "- 03: mean of absolute values of first derivatives of skin resistance \n",
    "- 04: mean first derivative for negative values only\n",
    "- 05: percentage of time with negative first derivative\n",
    "- 06: standard deviation of skin resistance\n",
    "- 14: number of local minima in the skin resistance signal\n",
    "\n",
    "Conductance\n",
    "- 07: number of local minima in the skin conductance signal\n",
    "- 10: standard deviation of skin conductance\n",
    "- 11: mean of first derivatives of skin conductance\n",
    "- 12: mean of absolute values of first derivatives of skin conductance \n",
    "- 13: mean of absolute values of second derivatives of skin conductance \n",
    "- 07: number of local minima in the skin conductance signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EDA_vs1': 0.055,\n",
       " 'EDA_vs2': 0.0,\n",
       " 'EDA_vs3': 0.0,\n",
       " 'EDA_vs4': 0.094,\n",
       " 'EDA_vs_max': 5.526912387473278,\n",
       " 'EDA_vs_min': 1.5890233091233288,\n",
       " 'EDA_vs_area': 0.151,\n",
       " 'EDA_vs_std': 1.9785259905152408,\n",
       " 'EDA_s1': 1.065,\n",
       " 'EDA_s2': 0.051,\n",
       " 'EDA_s3': 0.005,\n",
       " 'EDA_s4': 0.004,\n",
       " 'EDA_s5': 0.003,\n",
       " 'EDA_s6': 0.005,\n",
       " 'EDA_s7': 0.001,\n",
       " 'EDA_s8': 0.001,\n",
       " 'EDA_s9': 0.0,\n",
       " 'EDA_s10': 0.001,\n",
       " 'EDA_s_max': 7.570020981210178,\n",
       " 'EDA_s_min': 0.0011289742927930132,\n",
       " 'EDA_s_area': 1.253,\n",
       " 'EDA_s_std': 1.5602867909713318,\n",
       " 'EDA_r_mean': -0.12001356505197441,\n",
       " 'EDA_r_mean_1dev': 3.176472992819747e-07,\n",
       " 'EDA_r_mean_1dev_abs': 0.0014193898879788422,\n",
       " 'EDA_r_mean_1dev_neg': -0.0010786297316329198,\n",
       " 'EDA_r_mean_1dev_neg_perc': 0.657705532979056,\n",
       " 'EDA_r_std': 1.1029946901140928,\n",
       " 'EDA_r_loc_min': 217,\n",
       " 'EDA_avg_rise': 0.3466708346358237,\n",
       " 'EDA_c_loc_min': 220,\n",
       " 'EDA_c_std': 69.78487374061987,\n",
       " 'EDA_c_mean_1dev': -1.4864031866124008e-05,\n",
       " 'EDA_c_mean_1dev_abs': 0.11494806470336782,\n",
       " 'EDA_c_mean_2dev_abs': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1R1_EDA.EDA_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### 4\n",
    "\n",
    "As for ECG, also in EDA PSD were used to capture additional features. Once again, the reason behind is this choice is academical. PSD of EDA were used in related work in emotional state recognition (see report). Several researches have considered statistical aspects (minimum, maximum, and variance signal magnitude area, skewness, kurtosis, harmonics summation) to predict emotional state with EDA signal. Thus, these have been integrated in the array features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valence and Arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VA :\n",
    "    \n",
    "    def __init__(self, partic, clip):\n",
    "        \n",
    "        participant_data = data[partic-1]\n",
    "        \n",
    "        self.arousal = participant_data[\"recordings\"][clip][\"arousal\"]\n",
    "        \n",
    "        self.valence = participant_data[\"recordings\"][clip][\"valence\"]\n",
    "        \n",
    "    def get_arousal(self) :\n",
    "\n",
    "        return self.arousal\n",
    "    \n",
    "    def get_valence(self) :\n",
    "\n",
    "        return self.valence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers to questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson coefficient :  (-0.017677260717186068, 0.5004621723258766)\n"
     ]
    }
   ],
   "source": [
    "valence, arousal = [], []\n",
    "\n",
    "for i in range (1, 45) :\n",
    "    \n",
    "    for j in range(1, 37) : \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            PR_VA = VA(i, j)\n",
    "            arousal.append(PR_VA.get_arousal())\n",
    "            valence.append(PR_VA.get_valence())\n",
    "            \n",
    "        except KeyError:\n",
    "            \n",
    "              continue  \n",
    "                \n",
    "        except IndexError:\n",
    "            \n",
    "              continue  \n",
    "\n",
    "        \n",
    "print(\"Pearson coefficient : \", stats.pearsonr(np.array(valence), np.array(arousal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valence and arousal are uncorrellated, which means that valence is a bad predictor of arousal and vice versa. The two variables are independent to one another. This finding is not surprising because these two indicators measure two different phenomena. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_class</th>\n",
       "      <th>arousal_class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  valence_class arousal_class  count\n",
       "0          high          high    483\n",
       "1          high           low    188\n",
       "2           low          high    473\n",
       "3           low           low    311"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arousal_class, valence_class = [], []\n",
    "\n",
    "\n",
    "for i in range (0, len(valence)) :\n",
    "      \n",
    "    \n",
    "    if arousal[i]>3:\n",
    "        \n",
    "        \n",
    "        arousal_class.append(\"high\")\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        arousal_class.append(\"low\")   \n",
    "        \n",
    "        \n",
    "    if valence[i]>0:\n",
    "\n",
    "            \n",
    "        valence_class.append(\"high\")\n",
    "\n",
    "    else :\n",
    "        \n",
    "        valence_class.append(\"low\")\n",
    "\n",
    "        \n",
    "df=DataFrame(valence_class, columns=['valence_class'])\n",
    "\n",
    "df['arousal_class']=arousal_class\n",
    "\n",
    "df.groupby(['valence_class','arousal_class']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This emotion classification system is inspired by dimensional theories of emotions. \n",
    "\n",
    "For the majority of the cases (65%), the clips watched by the participants provoked high level of arousal . Researchers probably selected video clips with extreme images because they wanted to provoke distinguishable and intense emotions. Positive and negative emotions are equally distributed . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3621: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "final =[]\n",
    "\n",
    "features=[*[\"participant\", \"clip\"], *features, *[\"arousal\", \"valence\"]]\n",
    "\n",
    "final.append(features)\n",
    "\n",
    "for i in range (1, 45) :\n",
    "    \n",
    "    for j in range(1, 37) : \n",
    "       \n",
    "        try:\n",
    "            \n",
    "            PR_VA = VA(i, j)\n",
    "            \n",
    "            arousal = PR_VA.get_arousal()\n",
    "            \n",
    "            valence = PR_VA.get_valence()\n",
    "            \n",
    "            if arousal>3:\n",
    "                arousal=1 #high\n",
    "\n",
    "            else :\n",
    "                arousal=0 #low   \n",
    "\n",
    "\n",
    "            if valence>0:\n",
    "                valence=1 #high\n",
    "            else :\n",
    "                valence=0 #low\n",
    "            \n",
    "            ecg = list(ECG(i, j).ECG_stats().values())\n",
    "        \n",
    "            eeg = EEG_Features(i,j).get_x()\n",
    "            \n",
    "            emo = list(EMO(i, j).EMO_stats().values())\n",
    "            \n",
    "            eda = list(EDA(i, j).EDA_stats().values())\n",
    "\n",
    "\n",
    "            final.append([i, j,*ecg, *emo, *eda, *eeg, arousal, valence])\n",
    "\n",
    "        except KeyError:\n",
    "            \n",
    "              continue  \n",
    "                \n",
    "        except IndexError:\n",
    "            \n",
    "              continue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('featurelist', 'wb') as f:\n",
    "     pickle.dump(final, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('featurelist', 'rb') as f:\n",
    "      final = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNANwithMean(dataframe) :\n",
    "    \"\"\"Replace NaN with their Mean and zero infinitive values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : dataframe\n",
    "      dataframe of features and labels.\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    dataframe : dataframe\n",
    "      dataframe with only valid data.  \n",
    "    \"\"\"\n",
    "            \n",
    "    dataframe.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    keys = list(ECG(1,1).ECG_stats().keys())\n",
    "    \n",
    "    for key in keys : \n",
    "        \n",
    "        mean=dataframe[key].loc[dataframe[key] != \"nan\"].mean()\n",
    "        \n",
    "        dataframe[key].loc[(dataframe[key] == \"nan\")] = mean\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def leaveParticipant (dataframe, num) :\n",
    "    \n",
    "    \"\"\"LOPO validation scheme\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : dataframe\n",
    "      dataframe of features and labels.\n",
    "    num : int\n",
    "      participant to predict\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    X_train, X_test, y_train, y_test : 4 arrays (2x249-d arrays, 2x2-d arrays )\n",
    "      training and validation arrays \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    leaveParticipant = dataframe.loc[dataframe[\"participant\"] != num]\n",
    "\n",
    "    X_train = leaveParticipant.drop(\"arousal\", axis=1).drop(\"valence\", axis=1).to_numpy()\n",
    "    y_train = leaveParticipant[[\"arousal\",\"valence\" ]].to_numpy()\n",
    "\n",
    "    Participant = dataframe.loc[dataframe[\"participant\"] == num]\n",
    "    Participant = Participant.dropna().reset_index(drop=True)\n",
    "\n",
    "    X_test = Participant.drop(\"arousal\", axis=1).drop(\"valence\", axis=1).to_numpy()\n",
    "    y_test = Participant[[\"arousal\",\"valence\" ]].to_numpy()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "\n",
    "def leaveClip (dataframe, num) :\n",
    "    \n",
    "    \"\"\"LOCO validation scheme\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : dataframe\n",
    "      dataframe of features and labels.\n",
    "    num : int\n",
    "      clip to predict\n",
    "      \n",
    "    Return\n",
    "    ------\n",
    "    X_train, X_test, y_train, y_test : 4 arrays (2x249-d arrays, 2x2-d arrays )\n",
    "      training and validation arrays \n",
    "    \"\"\"\n",
    "    \n",
    "    #train\n",
    "    leaveClip = dataframe.loc[dataframe[\"clip\"] != num]\n",
    "    X_train = leaveClip.drop(\"arousal\", axis=1).drop(\"valence\", axis=1).drop(\"clip\", axis=1).drop(\"participant\", axis=1).to_numpy()\n",
    "    y_train = leaveClip[[\"arousal\",\"valence\" ]].to_numpy()\n",
    "\n",
    "    #test\n",
    "    Participant = dataframe.loc[dataframe[\"clip\"] == num]\n",
    "    X_test = Participant.drop(\"arousal\", axis=1).drop(\"valence\", axis=1).drop(\"clip\", axis=1).drop(\"participant\", axis=1).to_numpy()\n",
    "    y_test = Participant[[\"arousal\",\"valence\" ]].to_numpy()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(final[1:], columns=final[0])\n",
    "dataframe=replaceNANwithMean(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predictors & stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification :\n",
    "    \n",
    "    def __init__(self, dataframe, num, method) :\n",
    "        \n",
    "        self.num=num\n",
    "        \n",
    "        if (method == \"participant\") :\n",
    "            \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = leaveParticipant(dataframe, num)\n",
    "\n",
    "        else :\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = leaveClip(dataframe, num)\n",
    "\n",
    "            \n",
    "    def RandomForestClassifier(self) :\n",
    "\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "        ### Random Forest Classifier\n",
    "\n",
    "        clf=RandomForestClassifier(n_estimators=300)\n",
    "\n",
    "        clf.fit(self.X_train,self.y_train)\n",
    "\n",
    "        y_pred=clf.predict(self.X_test)\n",
    "\n",
    "        acc=round(accuracy_score(self.y_test, y_pred),2)\n",
    "        f1=round(f1_score(self.y_test, y_pred, average='micro'),2)\n",
    "        pre=round(precision_score(self.y_test, y_pred, average='micro'),2)\n",
    "        rec=round(recall_score(self.y_test, y_pred, average='micro'),2)\n",
    "        \n",
    "        arousal_cm=confusion_matrix([item[0] for item in self.y_test], [item[0] for item in y_pred], labels=[0,1])\n",
    "        \n",
    "        valence_cm=confusion_matrix([item[1] for item in self.y_test], [item[1] for item in y_pred], labels=[0,1])\n",
    "        \n",
    "        return [self.num, acc, f1, pre, rec, arousal_cm, valence_cm]\n",
    "    \n",
    "    \n",
    "    def ZeroRuleClassifier(self) :\n",
    "\n",
    "        from sklearn.dummy import DummyClassifier\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "        ### Zero Rule Classifier\n",
    "\n",
    "        dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "        dummy_clf.fit(self.X_train,self.y_train)\n",
    "\n",
    "        y_pred=dummy_clf.predict(self.X_test)\n",
    "\n",
    "        acc=round(accuracy_score(self.y_test, y_pred),2)\n",
    "        f1=round(f1_score(self.y_test, y_pred, average=\"micro\"),2)\n",
    "        pre=round(precision_score(self.y_test, y_pred, average=\"micro\"),2)\n",
    "        rec=round(recall_score(self.y_test, y_pred, average=\"micro\"),2)\n",
    "        \n",
    "        arousal_cm=confusion_matrix([item[0] for item in self.y_test], [item[0] for item in y_pred], labels=[0,1])\n",
    "        \n",
    "        valence_cm=confusion_matrix([item[1] for item in self.y_test], [item[1] for item in y_pred], labels=[0,1])\n",
    "\n",
    "        return [self.num, acc, f1, pre, rec, arousal_cm, valence_cm]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_part, zrc_part, rfc_clip, zrc_clip = [], [], [], []\n",
    "\n",
    "for i in range (1, 45) : \n",
    "    \n",
    "    test=classification(dataframe, i, \"participant\")\n",
    "    \n",
    "    rfc_part.append(test.RandomForestClassifier())\n",
    "    \n",
    "    zrc_part.append(test.ZeroRuleClassifier())\n",
    "    \n",
    "for i in range (1, 37) : \n",
    "    \n",
    "    test=classification(dataframe, i, \"clip\")\n",
    "    \n",
    "    rfc_clip.append(test.RandomForestClassifier())\n",
    "    \n",
    "    zrc_clip.append(test.ZeroRuleClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rfc_part', 'wb') as f:\n",
    "     pickle.dump(rfc_part, f) \n",
    "            \n",
    "with open('zrc_part', 'wb') as f:\n",
    "     pickle.dump(zrc_part, f) \n",
    "        \n",
    "with open('rfc_clip', 'wb') as f:\n",
    "     pickle.dump(rfc_clip, f) \n",
    "        \n",
    "with open('zrc_clip', 'wb') as f:\n",
    "     pickle.dump(zrc_clip, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('zrc_clip', 'rb') as f:\n",
    "      zrc_clip = pickle.load(f)\n",
    "        \n",
    "with open('rfc_part', 'rb') as f:\n",
    "      rfc_part = pickle.load(f)\n",
    "        \n",
    "with open('zrc_part', 'rb') as f:\n",
    "      zrc_part = pickle.load(f)\n",
    "        \n",
    "with open('rfc_clip', 'rb') as f:\n",
    "      rfc_clip = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Participant    Accuracy    F1    Precision    Recall  Confusion Matrix Arousal    Confusion Matrix Valence\n",
      "-------------  ----------  ----  -----------  --------  --------------------------  --------------------------\n",
      "            1        0.64  0.79         0.67      0.97  [[ 0 11]                    [[17  7]\n",
      "                                                         [ 0 25]]                    [ 1 11]]\n",
      "            2        0.75  0.9          0.85      0.96  [[ 0  7]                    [[16  1]\n",
      "                                                         [ 0 29]]                    [ 2 17]]\n",
      "            3        0.8   0.93         0.89      0.98  [[ 0  6]                    [[16  0]\n",
      "                                                         [ 0 29]]                    [ 1 18]]\n",
      "            4        0.44  0.73         0.59      0.94  [[ 0 18]                    [[16  4]\n",
      "                                                         [ 0 18]]                    [ 2 14]]\n",
      "            5        0.54  0.75         0.62      0.94  [[ 0 15]                    [[15  5]\n",
      "                                                         [ 0 20]]                    [ 2 13]]\n",
      "            6        0.76  0.91         0.85      0.98  [[ 0  6]                    [[15  2]\n",
      "                                                         [ 0 28]]                    [ 1 16]]\n",
      "            7        0.47  0.7          0.57      0.91  [[ 0 18]                    [[15  5]\n",
      "                                                         [ 0 18]]                    [ 3 13]]\n",
      "            8        0.47  0.79         0.67      0.97  [[ 0 18]                    [[17  0]\n",
      "                                                         [ 0 18]]                    [ 1 18]]\n",
      "            9        0.5   0.74         0.59      1     [[ 0  8]                    [[9 3]\n",
      "                                                         [ 0 10]]                    [0 6]]\n",
      "           10        0.69  0.89         0.85      0.94  [[ 0  7]                    [[15  1]\n",
      "                                                         [ 0 29]]                    [ 3 17]]\n",
      "           11        0.39  0.7          0.57      0.93  [[ 0 13]                    [[10  6]\n",
      "                                                         [ 0 15]]                    [ 2 10]]\n",
      "           12        0.42  0.71         0.56      0.97  [[ 0 18]                    [[17  6]\n",
      "                                                         [ 0 18]]                    [ 1 12]]\n",
      "           13        0.57  0.83         0.71      1     [[ 0 12]                    [[15  1]\n",
      "                                                         [ 0 18]]                    [ 0 14]]\n",
      "           14        0.61  0.82         0.7       0.97  [[ 0 14]                    [[17  2]\n",
      "                                                         [ 0 22]]                    [ 1 16]]\n",
      "           15        0.64  0.84         0.79      0.89  [[ 0  8]                    [[14  3]\n",
      "                                                         [ 0 28]]                    [ 5 14]]\n",
      "           16        0.57  0.78         0.71      0.87  [[ 0  7]                    [[ 4  4]\n",
      "                                                         [ 1 15]]                    [ 3 12]]\n",
      "           17        0.37  0.69         0.56      0.89  [[ 0 16]                    [[14  3]\n",
      "                                                         [ 0 14]]                    [ 3 10]]\n",
      "           18        0.58  0.8          0.7       0.92  [[ 0 13]                    [[16  3]\n",
      "                                                         [ 1 22]]                    [ 2 15]]\n",
      "           19        0.4   0.74         0.65      0.87  [[ 0 17]                    [[14  1]\n",
      "                                                         [ 0 18]]                    [ 5 15]]\n",
      "           20        0.58  0.79         0.67      0.95  [[ 1  8]                    [[17  9]\n",
      "                                                         [ 1 26]]                    [ 1  9]]\n",
      "           21        0.84  0.94         0.91      0.98  [[ 0  3]                    [[16  1]\n",
      "                                                         [ 0 28]]                    [ 1 13]]\n",
      "           22        0.47  0.77         0.62      1     [[ 0 15]                    [[16  3]\n",
      "                                                         [ 0 17]]                    [ 0 13]]\n",
      "           23        0.35  0.64         0.47      1     [[ 0 20]                    [[15  5]\n",
      "                                                         [ 0 11]]                    [ 0 11]]\n",
      "           24        0.64  0.83         0.83      0.83  [[ 0  4]                    [[9 1]\n",
      "                                                         [ 2 16]]                    [3 9]]\n",
      "           25        0.61  0.84         0.75      0.95  [[ 0 11]                    [[17  2]\n",
      "                                                         [ 0 25]]                    [ 2 15]]\n",
      "           26        0.52  0.8          0.76      0.83  [[ 0 10]                    [[13  1]\n",
      "                                                         [ 1 22]]                    [ 6 13]]\n",
      "           27        0.86  0.95         0.92      0.98  [[ 0  4]                    [[16  0]\n",
      "                                                         [ 0 31]]                    [ 1 18]]\n",
      "           28        0.64  0.79         0.67      0.97  [[ 0 12]                    [[17  6]\n",
      "                                                         [ 0 24]]                    [ 1 12]]\n",
      "           29        0.52  0.74         0.61      0.94  [[ 0 15]                    [[15  4]\n",
      "                                                         [ 0 18]]                    [ 2 12]]\n",
      "           30        0.47  0.73         0.61      0.89  [[ 0 14]                    [[14  7]\n",
      "                                                         [ 0 22]]                    [ 4 11]]\n",
      "           31        0.47  0.78         0.69      0.89  [[ 0 15]                    [[12  0]\n",
      "                                                         [ 0 17]]                    [ 4 16]]\n",
      "           32        0.56  0.81         0.75      0.88  [[ 0 11]                    [[16  2]\n",
      "                                                         [ 1 24]]                    [ 4 14]]\n",
      "           33        0.65  0.82         0.72      0.94  [[ 0  5]                    [[10  6]\n",
      "                                                         [ 0 21]]                    [ 2  8]]\n",
      "           34        0.47  0.79         0.85      0.74  [[ 0  7]                    [[ 3  1]\n",
      "                                                         [ 1 28]]                    [15 17]]\n",
      "           35        0.57  0.84         0.74      0.98  [[ 0 13]                    [[16  1]\n",
      "                                                         [ 0 22]]                    [ 1 17]]\n",
      "           36        0.58  0.81         0.75      0.88  [[ 1  8]                    [[15  5]\n",
      "                                                         [ 2 25]]                    [ 3 13]]\n",
      "           37        0.7   0.86         0.82      0.9   [[ 0  4]                    [[16  0]\n",
      "                                                         [ 0 16]]                    [ 2  2]]\n",
      "           38        0.61  0.83         0.74      0.95  [[ 0  7]                    [[8 0]\n",
      "                                                         [ 0 11]]                    [1 9]]\n",
      "           39        0.64  0.84         0.74      0.98  [[ 0 12]                    [[17  2]\n",
      "                                                         [ 0 24]]                    [ 1 16]]\n",
      "           40        0.95  0.98         0.96      1     [[ 0  0]                    [[14  1]\n",
      "                                                         [ 0 19]]                    [ 0  4]]\n",
      "           41        0.81  0.93         0.86      1     [[ 1  6]                    [[20  1]\n",
      "                                                         [ 0 29]]                    [ 0 15]]\n",
      "           42        0.56  0.76         0.62      0.97  [[ 0 15]                    [[18  5]\n",
      "                                                         [ 0 21]]                    [ 1 12]]\n",
      "           43        0.43  0.71         0.55      1     [[ 0 18]                    [[17  6]\n",
      "                                                         [ 0 17]]                    [ 0 12]]\n",
      "           44        0.53  0.76         0.63      0.97  [[ 0 14]                    [[17  6]\n",
      "                                                         [ 0 22]]                    [ 1 12]]\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "headers=['Participant','Accuracy', 'F1', 'Precision', 'Recall', 'Confusion Matrix Arousal', 'Confusion Matrix Valence']\n",
    "\n",
    "print(tabulate(rfc_part, headers= headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Clip    Accuracy    F1    Precision    Recall  Confusion Matrix Arousal    Confusion Matrix Valence\n",
      "------  ----------  ----  -----------  --------  --------------------------  --------------------------\n",
      "     1        0.15  0.59         0.61      0.58  [[ 0 15]                    [[ 7  4]\n",
      "                                                  [ 0 24]]                    [22  6]]\n",
      "     2        0.28  0.69         0.75      0.64  [[ 0 13]                    [[ 8  0]\n",
      "                                                  [ 1 26]]                    [20 12]]\n",
      "     3        0.26  0.68         0.68      0.68  [[ 0 14]                    [[10  2]\n",
      "                                                  [ 0 24]]                    [16 10]]\n",
      "     4        0.44  0.79         0.85      0.73  [[ 0  6]                    [[10  2]\n",
      "                                                  [ 0 35]]                    [17 12]]\n",
      "     5        0.44  0.82         0.95      0.73  [[ 0  2]                    [[ 2  1]\n",
      "                                                  [ 0 37]]                    [20 16]]\n",
      "     6        0.3   0.65         0.58      0.73  [[ 0 19]                    [[ 7  4]\n",
      "                                                  [ 0 18]]                    [12 14]]\n",
      "     7        0.31  0.72         0.74      0.7   [[ 0 12]                    [[ 7  2]\n",
      "                                                  [ 0 27]]                    [17 13]]\n",
      "     8        0.26  0.71         0.84      0.62  [[ 0  9]                    [[ 1  0]\n",
      "                                                  [ 0 33]]                    [28 13]]\n",
      "     9        0.38  0.79         0.87      0.72  [[ 0  6]                    [[ 7  1]\n",
      "                                                  [ 0 33]]                    [18 13]]\n",
      "    10        0.12  0.48         0.47      0.5   [[ 0 23]                    [[10  3]\n",
      "                                                  [ 0 18]]                    [23  5]]\n",
      "    11        0.18  0.55         0.64      0.49  [[ 0 16]                    [[ 3  0]\n",
      "                                                  [ 0 22]]                    [29  6]]\n",
      "    12        0.15  0.57         0.61      0.53  [[ 0 18]                    [[ 2  1]\n",
      "                                                  [ 0 21]]                    [27  9]]\n",
      "    13        0.18  0.5          0.5       0.51  [[ 0 24]                    [[ 1  2]\n",
      "                                                  [ 1 14]]                    [24 12]]\n",
      "    14        0.32  0.67         0.7       0.65  [[ 0 15]                    [[ 9  1]\n",
      "                                                  [ 0 26]]                    [20 11]]\n",
      "    15        0.25  0.59         0.55      0.63  [[ 0 19]                    [[10  2]\n",
      "                                                  [ 0 17]]                    [15  9]]\n",
      "    16        0.2   0.67         0.78      0.59  [[ 0 11]                    [[ 1  0]\n",
      "                                                  [ 1 28]]                    [27 12]]\n",
      "    17        0.1   0.62         0.78      0.52  [[ 0  9]                    [[ 3  1]\n",
      "                                                  [ 0 31]]                    [32  4]]\n",
      "    18        0.15  0.47         0.45      0.5   [[ 0 27]                    [[ 8  1]\n",
      "                                                  [ 0 14]]                    [23  9]]\n",
      "    19        0.2   0.46         0.33      0.73  [[ 0 25]                    [[28  7]\n",
      "                                                  [ 0 16]]                    [ 6  0]]\n",
      "    20        0.5   0.68         0.54      0.94  [[ 0 14]                    [[26 12]\n",
      "                                                  [ 0 28]]                    [ 2  2]]\n",
      "    21        0.49  0.68         0.53      0.96  [[ 0 16]                    [[32  7]\n",
      "                                                  [ 0 25]]                    [ 1  1]]\n",
      "    22        0.4   0.67         0.53      0.9   [[ 0 17]                    [[28  8]\n",
      "                                                  [ 0 25]]                    [ 3  3]]\n",
      "    23        0.34  0.51         0.35      0.95  [[ 0 24]                    [[29 10]\n",
      "                                                  [ 0 17]]                    [ 1  1]]\n",
      "    24        0.22  0.52         0.43      0.66  [[ 0 25]                    [[16  6]\n",
      "                                                  [ 0 16]]                    [12  7]]\n",
      "    25        0.61  0.8          0.71      0.91  [[ 0  8]                    [[27  4]\n",
      "                                                  [ 0 28]]                    [ 3  2]]\n",
      "    26        0.45  0.66         0.52      0.9   [[ 1 14]                    [[22 10]\n",
      "                                                  [ 0 23]]                    [ 3  3]]\n",
      "    27        0.49  0.71         0.6       0.87  [[ 0 13]                    [[29  5]\n",
      "                                                  [ 0 26]]                    [ 4  1]]\n",
      "    28        0.49  0.74         0.59      1     [[ 0 10]                    [[27 11]\n",
      "                                                  [ 0 29]]                    [ 0  1]]\n",
      "    29        0.46  0.66         0.51      0.92  [[ 1 15]                    [[32  8]\n",
      "                                                  [ 2 23]]                    [ 0  1]]\n",
      "    30        0.5   0.77         0.64      0.97  [[ 0  8]                    [[27 12]\n",
      "                                                  [ 0 34]]                    [ 1  2]]\n",
      "    31        0.7   0.85         0.77      0.95  [[ 0  4]                    [[31  7]\n",
      "                                                  [ 1 35]]                    [ 1  1]]\n",
      "    32        0.7   0.85         0.77      0.95  [[ 0  4]                    [[31  7]\n",
      "                                                  [ 1 35]]                    [ 1  1]]\n",
      "    33        0.56  0.81         0.8       0.82  [[ 0  5]                    [[24  4]\n",
      "                                                  [ 0 34]]                    [ 8  3]]\n",
      "    34        0.38  0.65         0.49      0.96  [[ 0 11]                    [[18 17]\n",
      "                                                  [ 0 26]]                    [ 1  1]]\n",
      "    35        0.5   0.72         0.59      0.91  [[ 0 10]                    [[24 10]\n",
      "                                                  [ 0 28]]                    [ 3  1]]\n",
      "    36        0.57  0.8          0.67      1     [[ 0  3]                    [[23 16]\n",
      "                                                  [ 0 37]]                    [ 0  1]]\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "headers=['Clip','Accuracy', 'F1', 'Precision', 'Recall', 'Confusion Matrix Arousal', 'Confusion Matrix Valence']\n",
    "\n",
    "print(tabulate(rfc_clip, headers= headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9431cb2b14534b77a3d62236362f23a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc77195cc1e4dc2b9ef6aa7169a7522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fa1113b2040>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "arousal=list(map(list, zip(*rfc_part)))[-2]\n",
    "valence=list(map(list, zip(*rfc_part)))[-1]\n",
    "\n",
    "sum_cf_ar = np.zeros((2, 2))\n",
    "sum_cf_va = np.zeros((2, 2))\n",
    "\n",
    "for i in range (0,len(arousal)):\n",
    "    sum_cf_ar=sum_cf_ar+arousal[i]\n",
    "    sum_cf_va=sum_cf_va+valence[i]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=sum_cf_ar, display_labels=[0,1])\n",
    "disp.plot(values_format=\".1f\", cmap=plt.cm.Blues,) \n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=sum_cf_va, display_labels=[0,1])\n",
    "disp.plot(values_format=\".1f\", cmap=plt.cm.Blues,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f8dd74c49f49518110566728e994d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c0069ccfe74feda8ce2bfbb1d8ac74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fa112fdd2b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arousal=list(map(list, zip(*rfc_clip)))[-2]\n",
    "valence=list(map(list, zip(*rfc_clip)))[-1]\n",
    "\n",
    "sum_cf_ar = np.zeros((2, 2))\n",
    "sum_cf_va = np.zeros((2, 2))\n",
    "\n",
    "for i in range (0,len(arousal)):\n",
    "    sum_cf_ar=sum_cf_ar+arousal[i]\n",
    "    sum_cf_va=sum_cf_va+valence[i]\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=sum_cf_ar, display_labels=[0,1])\n",
    "disp.plot(values_format=\".1f\", cmap=plt.cm.Blues,) \n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=sum_cf_va, display_labels=[0,1])\n",
    "disp.plot(values_format=\".1f\", cmap=plt.cm.Blues,) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 features for arousal prediction :  valence           0.120789\n",
      "EMO_vdLL_PERQ1    0.102161\n",
      "EMO_hdLLC_std     0.101611\n",
      "clip              0.076309\n",
      "EDA_vs_area       0.065592\n",
      "EMO_dLE_mean      0.064913\n",
      "EEG_15            0.059638\n",
      "EEG_8             0.058480\n",
      "EEG_12            0.056818\n",
      "EEG_13            0.056427\n",
      "Name: arousal, dtype: float64\n",
      "10 features for valence prediction :  clip              0.602941\n",
      "arousal           0.120789\n",
      "EEG_19            0.118785\n",
      "EMO_vdLL_PERQ3    0.099383\n",
      "EEG_32            0.089235\n",
      "EEG_62            0.071926\n",
      "EEG_76            0.071193\n",
      "EEG_80            0.070698\n",
      "EMO_vdLL_mean     0.070512\n",
      "EEG_77            0.070239\n",
      "Name: valence, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = dataframe.corr()\n",
    "pd.set_option('display.max_rows', 10)\n",
    "top10Aro=corr_matrix[\"arousal\"].abs().sort_values(ascending=False)[1:11]\n",
    "top10Val=corr_matrix[\"valence\"].abs().sort_values(ascending=False)[1:11]\n",
    "\n",
    "print(\"10 features for arousal prediction : \", top10Aro)\n",
    "print(\"10 features for valence prediction : \", top10Val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier       Accuracy    F1    Precision    Recall\n",
      "-------------  ----------  ----  -----------  --------\n",
      "Random Forest        0.57   0.8         0.7       0.95\n",
      "Zero Rule            0.33   0.6         0.66      0.59\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "headers=['Classifier', 'Accuracy', 'F1', 'Precision', 'Recall']\n",
    "\n",
    "part, list_rfc, list_zrc = [], ['Random Forest'], ['Zero Rule']\n",
    "\n",
    "list_rfc.append(round(statistics.median([item[1] for item in rfc_part]),2))\n",
    "list_rfc.append(round(statistics.median([item[2] for item in rfc_part]),2))\n",
    "list_rfc.append(round(statistics.median([item[3] for item in rfc_part]),2))\n",
    "list_rfc.append(round(statistics.median([item[4] for item in rfc_part]),2))\n",
    "\n",
    "list_zrc.append(round(statistics.median([item[1] for item in zrc_part]),2))\n",
    "list_zrc.append(round(statistics.median([item[2] for item in zrc_part]),2))\n",
    "list_zrc.append(round(statistics.median([item[3] for item in zrc_part]),2))\n",
    "list_zrc.append(round(statistics.median([item[4] for item in zrc_part]),2))\n",
    "\n",
    "part.append(list_rfc)\n",
    "part.append(list_zrc)  \n",
    "\n",
    "print(tabulate(part, headers= headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier       Accuracy    F1    Precision    Recall\n",
      "-------------  ----------  ----  -----------  --------\n",
      "Random Forest        0.36  0.68         0.61      0.73\n",
      "Zero Rule            0.21  0.61         0.67      0.54\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "headers=['Classifier', 'Accuracy', 'F1', 'Precision', 'Recall']\n",
    "\n",
    "part, list_rfc, list_zrc = [], ['Random Forest'], ['Zero Rule']\n",
    "\n",
    "list_rfc.append(round(statistics.median([item[1] for item in rfc_clip]),2))\n",
    "list_rfc.append(round(statistics.median([item[2] for item in rfc_clip]),2))\n",
    "list_rfc.append(round(statistics.median([item[3] for item in rfc_clip]),2))\n",
    "list_rfc.append(round(statistics.median([item[4] for item in rfc_clip]),2))\n",
    "\n",
    "list_zrc.append(round(statistics.median([item[1] for item in zrc_clip]),2))\n",
    "list_zrc.append(round(statistics.median([item[2] for item in zrc_clip]),2))\n",
    "list_zrc.append(round(statistics.median([item[3] for item in zrc_clip]),2))\n",
    "list_zrc.append(round(statistics.median([item[4] for item in zrc_clip]),2))\n",
    "\n",
    "part.append(list_rfc)\n",
    "part.append(list_zrc)  \n",
    "\n",
    "print(tabulate(part, headers= headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code - References \n",
    "\n",
    "[1] Mojtaba Khomami Abadi, Ramanathan Subramanian, Seyed Mostafa Kia, Paolo Avesani, Ioannis Patras, and Nicu Sebe. Decaf: Meg-based multimodal database for decoding affective physiolog- ical responses. IEEE Transactions on Affective Computing, 6(3):209–222, 2015.\n",
    "\n",
    "[2] http://ems12lead.com/2014/03/10/understanding-ecg-filtering/\n",
    "\n",
    "[3] http://www.jscholaronline.org/articles/JBER/Signal-Processing.pdf\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
